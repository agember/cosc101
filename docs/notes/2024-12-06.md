# Machine learning: mathematics
_COSC 101, Introduction to Computing I, 2024-12-06_

## Announcements
* Exam 3 reattempts Wednesday 11:20am-11:45am
    * No need to attend class on Wednesday if you do not need to reattempt any questions
* Revisions to homeworks 5, 6, and/or 7 due Thursday @ 11:59pm

## Warm-up

_Write a function called `predict_fly` that takes a dictionary of boolean features (e.g., `{'animal': True, 'wings': False, 'heavy': True, 'propeller': False}`) and implements your decision tree from last class to predict whether or not the thing flies (i.e., return `True` or `False`)._


```python
def predict_fly(features: dict) -> bool:
    """
    >>> predict_fly({'animal': True, 'wings': False, 'heavy': True, 'propeller': False})
    False
    >>> predict_fly({'animal': False, 'wings': True, 'heavy': True, 'propeller': False})
    True
    >>> predict_fly({'animal': False, 'wings': False, 'heavy': True, 'propeller': False})
    False
    """
    if features["animal"]:
        if features["wings"]:
            if features["heavy"]:
                return False
            else:
                return True
        else:
            return False
    else:
        if features["wings"]:
            return True
        else:
            if features["propeller"]:
                return True
            else:
                return False
```

<p style="height:47em;"></p>

## Training data

* _Why aren't the predictions correct?_
    * Not enough features
    * Too few training samples
    * Biased training data

1. _Add to our training data a feature you used in your decision tree_
2. _Add an additional thing to our training data along with its features_

<img src="training_data_sheet.png" style="width:200px;"/>

## Training

* How can we automate the process of building a decision tree?
* Recall: _What made a question a good or bad question?_ – separated things into more "pure" groups
* Information gain – measure of reduction in uncertainty
* Uncertainty is measured based the probability of a thing belonging to one group versus another
    * Example
        * Assume we have 4 things that fly and 2 things that don't fly
        * The probability of picking a thing that flies is 4/6 and the probability of picking a thing that doesn't fly is 2/6
        * The uncertainty is -4/6 * log2 4/6 + -2/6 * log2 2/6 = 0.918
    * If everything is in the same group then uncertainty is 0
    * If there are an equal number of things in each group, then uncertainty is 1
* Information gain is measured based on the uncertainty of a group before dividing it based on a feature compared to the uncertainty of the two groups when divided based on a feature
    * Example
        * If we have 4 things that fly and 2 things that don't fly, then the uncertainty is 0.918
        * Divide into groups based on whether or not they have wings – assume 3 of the 4 things that fly have wings and none of the things that don't fly have wings
        * For the group of things with wings, all fly so the uncertainty is 0
        * For the group of things without wings, 1 flies and 2 don't fly, so the uncertainty is -1/3 * log2 1/3 + -2/3 * log2 2/3 = 0.918
        * Compute the weighted average to compute the new amount of uncertainty: 3/6 * 0 + 3/6 * 0.918 = 0.459
        * Information gain is original uncertainty - new uncertainty: 0.981 - 0.459 = 0.459
* For a group, compute information gain for each feature; choose feature with highest information gain


```python
import doctest
doctest.testmod()
```




    TestResults(failed=0, attempted=3)


